{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/1003874/anaconda/envs/lstm/lib/python3.6/site-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "from jamo import j2hcj, h2j\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from konlpy.tag import Twitter\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "twitter = Twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/1003874/anaconda/envs/lstm/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\n",
    "from tensorflow.contrib.rnn import BasicLSTMCell\n",
    "from utils.prepare_data import *\n",
    "import time\n",
    "from utils.model_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testcase_shuffle_data_load():\n",
    "    df = pd.read_csv('/Users/1003874/bsdev/Text-classification-with-CNN-RNN-with-Tensorflow/Ch01_Data_load/data/preprocessed_mart_digi_speech_act.csv')\n",
    "    df['clean_text'] = df['clean_text'].apply(lambda x:x.lower())\n",
    "    s = df['class'].value_counts()\n",
    "    cols = s[s>50].index.tolist()\n",
    "\n",
    "    sub_list = []\n",
    "    for col in cols:\n",
    "        sub_list.append(df[df['class'] == col])\n",
    "    df = pd.concat(sub_list, axis=0)\n",
    "    df = df.sample(len(df), random_state=10).reset_index(drop=True)\n",
    "\n",
    "    added_df = pd.read_csv('/Users/1003874/bsdev/Text-classification-with-CNN-RNN-with-Tensorflow/Ch01_Data_load/data/added_dataset.csv',sep='\\t')\n",
    "    added_df['clean_text'] = added_df['sent'].apply(lambda x:x.lower())\n",
    "\n",
    "    # class 제거\n",
    "    added_df = added_df[added_df['class'].apply(lambda x: True if x in cols else False)]\n",
    "    added_df.info()\n",
    "\n",
    "    # duplicated sent 제거\n",
    "    added_df = added_df[added_df['clean_text'].apply(lambda x: False if x in df['clean_text'].tolist() else True)]\n",
    "    added_df.info()\n",
    "\n",
    "    # train_df merge\n",
    "\n",
    "    size = len(df)\n",
    "    rate = 0.2\n",
    "    train_df = df.iloc[:int(size * (1 - rate))]\n",
    "    test_df = df.iloc[int(size * (1 - rate)):]\n",
    "\n",
    "    all_df = pd.concat([train_df[['clean_text', 'class']],\n",
    "                        test_df[['clean_text', 'class']],\n",
    "                        added_df[['clean_text', 'class']]])\n",
    "\n",
    "    all_df = all_df.sample(len(all_df), random_state=10)\n",
    "    all_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    label_df = pd.get_dummies(all_df['class'])\n",
    "\n",
    "    train_df = all_df.iloc[1062:]\n",
    "    test_df = all_df.iloc[:1062]\n",
    "\n",
    "    cols = label_df.columns\n",
    "\n",
    "    print(len(train_df), len(test_df))\n",
    "\n",
    "    TRAIN_DOC, TRAIN_LABEL, TEST_DOC, TEST_LABEL = \\\n",
    "        train_df['clean_text'].values, label_df.loc[train_df.index].values, test_df['clean_text'].values, label_df.loc[\n",
    "            test_df.index].values\n",
    "\n",
    "    return TRAIN_DOC, TRAIN_LABEL, TEST_DOC, TEST_LABEL, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testcase_add_data_load():\n",
    "    df = pd.read_csv('/Users/1003874/bsdev/Text-classification-with-CNN-RNN-with-Tensorflow/Ch01_Data_load/data/preprocessed_mart_digi_speech_act.csv')\n",
    "    df['clean_text'] = df['clean_text'].apply(lambda x:x.lower())\n",
    "    s = df['class'].value_counts()\n",
    "    cols = s[s>50].index.tolist()\n",
    "\n",
    "    sub_list = []\n",
    "    for col in cols:\n",
    "        sub_list.append(df[df['class'] == col])\n",
    "    df = pd.concat(sub_list, axis=0)\n",
    "    df = df.sample(len(df), random_state=10).reset_index(drop=True)\n",
    "\n",
    "    added_df = pd.read_csv(\n",
    "        '/Users/1003874/bsdev/Text-classification-with-CNN-RNN-with-Tensorflow/Ch01_Data_load/data/added_dataset.csv',\n",
    "        sep='\\t')\n",
    "    added_df['clean_text'] = added_df['sent'].apply(lambda x:x.lower())\n",
    "\n",
    "    # class 제거\n",
    "    added_df = added_df[added_df['class'].apply(lambda x: True if x in cols else False)]\n",
    "    added_df.info()\n",
    "\n",
    "    # duplicated sent 제거\n",
    "    added_df = added_df[added_df['clean_text'].apply(lambda x: False if x in df['clean_text'].tolist() else True)]\n",
    "    added_df.info()\n",
    "\n",
    "    # train_df merge\n",
    "\n",
    "    size = len(df)\n",
    "    rate = 0.2\n",
    "    train_df = df.iloc[:int(size * (1 - rate))]\n",
    "    test_df = df.iloc[int(size * (1 - rate)):]\n",
    "    train_df['setlabel'] = 1\n",
    "    test_df['setlabel'] = 0\n",
    "    added_df['setlabel'] = 2\n",
    "\n",
    "    all_df = pd.concat([train_df[['clean_text', 'class', 'setlabel']],\n",
    "                        test_df[['clean_text', 'class', 'setlabel']],\n",
    "                        added_df[['clean_text', 'class','setlabel']]])\n",
    "\n",
    "    all_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    label_df = pd.get_dummies(all_df['class'])\n",
    "\n",
    "    train_df = all_df[all_df['setlabel'] != 0]\n",
    "    test_df = all_df[all_df['setlabel'] == 0]\n",
    "\n",
    "    cols = label_df.columns\n",
    "\n",
    "    print(len(train_df), len(test_df))\n",
    "\n",
    "    TRAIN_DOC, TRAIN_LABEL, TEST_DOC, TEST_LABEL = \\\n",
    "        train_df['clean_text'].values, label_df.loc[train_df.index].values, test_df['clean_text'].values, label_df.loc[\n",
    "            test_df.index].values\n",
    "\n",
    "    return TRAIN_DOC, TRAIN_LABEL, TEST_DOC, TEST_LABEL, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def digi_data_load():\n",
    "    df = pd.read_csv('/Users/1003874/bsdev/Text-classification-with-CNN-RNN-with-Tensorflow/Ch01_Data_load/data/preprocessed_mart_digi_speech_act.csv')\n",
    "    df['clean_text'] = df['clean_text'].apply(lambda x:x.lower())\n",
    "    s = df['class'].value_counts()\n",
    "    cols = s[s>50].index.tolist()\n",
    "\n",
    "    sub_list = []\n",
    "    for col in cols:\n",
    "        sub_list.append(df[df['class'] == col])\n",
    "    df = pd.concat(sub_list, axis=0)\n",
    "    df = df.sample(len(df), random_state=10).reset_index(drop=True)\n",
    "\n",
    "    size = len(df)\n",
    "    rate = 0.2\n",
    "    train_df = df.iloc[:int(size*(1-rate))]\n",
    "    test_df = df.iloc[int(size*(1-rate)):]\n",
    "    label_df = pd.get_dummies(df['class'])\n",
    "    cols = label_df.columns\n",
    "    print(len(train_df), len(test_df))\n",
    "\n",
    "    TRAIN_DOC, TRAIN_LABEL, TEST_DOC, TEST_LABEL = \\\n",
    "        train_df['clean_text'].values, label_df.loc[train_df.index].values, test_df['clean_text'].values, label_df.loc[test_df.index].values\n",
    "\n",
    "    return TRAIN_DOC, TRAIN_LABEL, TEST_DOC, TEST_LABEL, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ABLSTM(object):\n",
    "    def __init__(self, config):\n",
    "        self.max_len = config[\"max_len\"]\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.vocab_size = config[\"vocab_size\"]\n",
    "        self.embedding_size = config[\"embedding_size\"]\n",
    "        self.n_class = config[\"n_class\"]\n",
    "        self.learning_rate = config[\"learning_rate\"]\n",
    "\n",
    "        # placeholder\n",
    "        self.x = tf.placeholder(tf.int32, [None, self.max_len])\n",
    "        self.label = tf.placeholder(tf.int32, [None])\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    def build_graph(self):\n",
    "        print(\"building graph\")\n",
    "        # Word embedding\n",
    "        embeddings_var = tf.Variable(tf.random_uniform([self.vocab_size, self.embedding_size], -1.0, 1.0),\n",
    "                                     trainable=True)\n",
    "        batch_embedded = tf.nn.embedding_lookup(embeddings_var, self.x)\n",
    "\n",
    "        rnn_outputs, _ = bi_rnn(BasicLSTMCell(self.hidden_size),\n",
    "                                BasicLSTMCell(self.hidden_size),\n",
    "                                inputs=batch_embedded, dtype=tf.float32)\n",
    "\n",
    "        fw_outputs, bw_outputs = rnn_outputs\n",
    "\n",
    "        W = tf.Variable(tf.random_normal([self.hidden_size], stddev=0.1))\n",
    "        H = fw_outputs + bw_outputs  # (batch_size, seq_len, HIDDEN_SIZE)\n",
    "        M = tf.tanh(H)  # M = tanh(H)  (batch_size, seq_len, HIDDEN_SIZE)\n",
    "\n",
    "        self.alpha = tf.nn.softmax(tf.reshape(tf.matmul(tf.reshape(M, [-1, self.hidden_size]),\n",
    "                                                        tf.reshape(W, [-1, 1])),\n",
    "                                              (-1, self.max_len)))  # batch_size x seq_len\n",
    "        r = tf.matmul(tf.transpose(H, [0, 2, 1]),\n",
    "                      tf.reshape(self.alpha, [-1, self.max_len, 1]))\n",
    "        r = tf.squeeze(r)\n",
    "        h_star = tf.tanh(r)  # (batch , HIDDEN_SIZE\n",
    "\n",
    "        h_drop = tf.nn.dropout(h_star, self.keep_prob)\n",
    "\n",
    "        # Fully connected layer（dense layer)\n",
    "        FC_W = tf.Variable(tf.truncated_normal([self.hidden_size, self.n_class], stddev=0.1))\n",
    "        FC_b = tf.Variable(tf.constant(0., shape=[self.n_class]))\n",
    "        y_hat = tf.nn.xw_plus_b(h_drop, FC_W, FC_b)\n",
    "\n",
    "        self.loss = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y_hat, labels=self.label))\n",
    "\n",
    "        # prediction\n",
    "        self.prediction = tf.argmax(tf.nn.softmax(y_hat), 1)\n",
    "\n",
    "        # optimization\n",
    "        loss_to_minimize = self.loss\n",
    "        tvars = tf.trainable_variables()\n",
    "        gradients = tf.gradients(loss_to_minimize, tvars, aggregation_method=tf.AggregationMethod.EXPERIMENTAL_TREE)\n",
    "        grads, global_norm = tf.clip_by_global_norm(gradients, 1.0)\n",
    "\n",
    "        self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        self.train_op = self.optimizer.apply_gradients(zip(grads, tvars), global_step=self.global_step,\n",
    "                                                       name='train_step')\n",
    "        print(\"graph built successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4244 1062\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DOC, TRAIN_LABEL, TEST_DOC, TEST_LABEL, cols = digi_data_load()\n",
    "#TRAIN_DOC, TRAIN_LABEL, TEST_DOC, TEST_LABEL, cols = testcase_add_data_load()\n",
    "#TRAIN_DOC, TRAIN_LABEL, TEST_DOC, TEST_LABEL, cols = testcase_shuffle_data_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_intent = np.apply_along_axis(np.argmax,1,TRAIN_LABEL)\n",
    "test_intent = np.apply_along_axis(np.argmax,1,TEST_LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 417 ms, sys: 5.16 ms, total: 422 ms\n",
      "Wall time: 421 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_doc = TRAIN_DOC.tolist()\n",
    "test_doc = TEST_DOC.tolist()\n",
    "\n",
    "#train_doc = pd.Series(list(map(lambda x: ' '.join(twitter.morphs(x)), train_doc)))\n",
    "#test_doc = pd.Series(list(map(lambda x: ' '.join(twitter.morphs(x)), test_doc)))\n",
    "train_doc = pd.Series(list(map(lambda x: j2hcj(h2j(x)), train_doc)))\\\n",
    "    .apply(lambda x: ' '.join(list(x.replace(' ','ⓢ'))))\n",
    "test_doc = pd.Series(list(map(lambda x: j2hcj(h2j(x)), test_doc)))\\\n",
    "    .apply(lambda x: ' '.join(list(x.replace(' ','ⓢ'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = pd.Series(train_intent)\n",
    "y_test = pd.Series(test_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, tokenizer = \\\n",
    "        data_preprocessing_v2(train_doc, test_doc, max_len=128,max_words=500)\n",
    "vocab_size= tokenizer.num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 1,\n",
       " '1': 84,\n",
       " '2': 3,\n",
       " '4': 2,\n",
       " '5': 2,\n",
       " 'a': 82,\n",
       " 'b': 1,\n",
       " 'c': 6,\n",
       " 'd': 4,\n",
       " 'e': 6,\n",
       " 'f': 5,\n",
       " 'g': 1,\n",
       " 'h': 1,\n",
       " 'i': 38,\n",
       " 'k': 31,\n",
       " 'l': 4,\n",
       " 'm': 8,\n",
       " 'n': 4,\n",
       " 'o': 20,\n",
       " 'p': 57,\n",
       " 'q': 7,\n",
       " 'r': 4,\n",
       " 's': 64,\n",
       " 't': 25,\n",
       " 'u': 3,\n",
       " 'v': 27,\n",
       " 'w': 1,\n",
       " 'y': 25,\n",
       " 'ⓢ': 3544,\n",
       " 'ㄱ': 2808,\n",
       " 'ㄲ': 234,\n",
       " 'ㄴ': 3476,\n",
       " 'ㄶ': 76,\n",
       " 'ㄷ': 1966,\n",
       " 'ㄸ': 325,\n",
       " 'ㄹ': 2232,\n",
       " 'ㄺ': 1,\n",
       " 'ㅀ': 6,\n",
       " 'ㅁ': 2268,\n",
       " 'ㅂ': 1712,\n",
       " 'ㅃ': 34,\n",
       " 'ㅄ': 113,\n",
       " 'ㅅ': 2218,\n",
       " 'ㅆ': 628,\n",
       " 'ㅇ': 3829,\n",
       " 'ㅈ': 2046,\n",
       " 'ㅉ': 51,\n",
       " 'ㅊ': 682,\n",
       " 'ㅋ': 607,\n",
       " 'ㅌ': 516,\n",
       " 'ㅍ': 1097,\n",
       " 'ㅎ': 1788,\n",
       " 'ㅏ': 3430,\n",
       " 'ㅐ': 1322,\n",
       " 'ㅑ': 152,\n",
       " 'ㅒ': 6,\n",
       " 'ㅓ': 2282,\n",
       " 'ㅔ': 1571,\n",
       " 'ㅕ': 906,\n",
       " 'ㅖ': 95,\n",
       " 'ㅗ': 2234,\n",
       " 'ㅘ': 540,\n",
       " 'ㅙ': 129,\n",
       " 'ㅚ': 434,\n",
       " 'ㅛ': 1610,\n",
       " 'ㅜ': 2084,\n",
       " 'ㅝ': 354,\n",
       " 'ㅞ': 10,\n",
       " 'ㅟ': 229,\n",
       " 'ㅠ': 93,\n",
       " 'ㅡ': 2082,\n",
       " 'ㅢ': 191,\n",
       " 'ㅣ': 2619}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size:  1062\n",
      "building graph\n",
      "graph built successfully!\n"
     ]
    }
   ],
   "source": [
    "x_dev = x_test\n",
    "y_dev = y_test\n",
    "dev_size = len(x_dev)\n",
    "test_size = len(x_test)\n",
    "print(\"Validation Size: \", dev_size)\n",
    "\n",
    "config = {\n",
    "    \"max_len\": 128,\n",
    "    \"hidden_size\": 64,\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"embedding_size\": 64,\n",
    "    \"n_class\": 33,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"batch_size\": 4,\n",
    "    \"train_epoch\": 20\n",
    "}\n",
    "\n",
    "classifier = ABLSTM(config)\n",
    "classifier.build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 start !\n",
      "Train Epoch time:  60.779 s\n",
      "validation accuracy: 0.284 \n",
      "Epoch 2 start !\n",
      "Train Epoch time:  63.751 s\n",
      "validation accuracy: 0.407 \n",
      "Epoch 3 start !\n",
      "Train Epoch time:  63.705 s\n",
      "validation accuracy: 0.458 \n",
      "Epoch 4 start !\n",
      "Train Epoch time:  68.619 s\n",
      "validation accuracy: 0.519 \n",
      "Epoch 5 start !\n",
      "Train Epoch time:  66.110 s\n",
      "validation accuracy: 0.526 \n",
      "Epoch 6 start !\n",
      "Train Epoch time:  61.886 s\n",
      "validation accuracy: 0.546 \n",
      "Epoch 7 start !\n",
      "Train Epoch time:  67.526 s\n",
      "validation accuracy: 0.568 \n",
      "Epoch 8 start !\n",
      "Train Epoch time:  66.166 s\n",
      "validation accuracy: 0.574 \n",
      "Epoch 9 start !\n",
      "Train Epoch time:  63.638 s\n",
      "validation accuracy: 0.589 \n",
      "Epoch 10 start !\n",
      "Train Epoch time:  64.058 s\n",
      "validation accuracy: 0.570 \n",
      "Epoch 11 start !\n",
      "Train Epoch time:  63.254 s\n",
      "validation accuracy: 0.603 \n",
      "Epoch 12 start !\n",
      "Train Epoch time:  61.218 s\n",
      "validation accuracy: 0.609 \n",
      "Epoch 13 start !\n",
      "Train Epoch time:  63.576 s\n",
      "validation accuracy: 0.615 \n",
      "Epoch 14 start !\n",
      "Train Epoch time:  65.016 s\n",
      "validation accuracy: 0.613 \n",
      "Epoch 15 start !\n",
      "Train Epoch time:  65.790 s\n",
      "validation accuracy: 0.598 \n",
      "Epoch 16 start !\n",
      "Train Epoch time:  62.357 s\n",
      "validation accuracy: 0.609 \n",
      "Epoch 17 start !\n",
      "Train Epoch time:  64.123 s\n",
      "validation accuracy: 0.608 \n",
      "Epoch 18 start !\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "dev_batch = (x_dev, y_dev)\n",
    "start = time.time()\n",
    "for e in range(config[\"train_epoch\"]):\n",
    "\n",
    "    t0 = time.time()\n",
    "    print(\"Epoch %d start !\" % (e + 1))\n",
    "    for x_batch, y_batch in fill_feed_dict(x_train, y_train, config[\"batch_size\"]):\n",
    "        return_dict = run_train_step(classifier, sess, (x_batch, y_batch))\n",
    "        attn = get_attn_weight(classifier, sess, (x_batch, y_batch))\n",
    "        # plot the attention weight\n",
    "        # print(np.reshape(attn, (config[\"batch_size\"], config[\"max_len\"])))\n",
    "    t1 = time.time()\n",
    "\n",
    "    print(\"Train Epoch time:  %.3f s\" % (t1 - t0))\n",
    "    dev_acc = run_eval_step(classifier, sess, dev_batch)\n",
    "    print(\"validation accuracy: %.3f \" % dev_acc)\n",
    "\n",
    "print(\"Training finished, time consumed : \", time.time() - start, \" s\")\n",
    "print(\"Start evaluating:  \\n\")\n",
    "cnt = 0\n",
    "test_acc = 0\n",
    "for x_batch, y_batch in fill_feed_dict(x_test, y_test, config[\"batch_size\"]):\n",
    "    acc = run_eval_step(classifier, sess, (x_batch, y_batch))\n",
    "    test_acc += acc\n",
    "    cnt += 1\n",
    "\n",
    "print(\"Test accuracy : %f %%\" % (test_acc / cnt * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "reals = []\n",
    "attns = []\n",
    "for x_batch, y_batch in fill_feed_dict(x_test, y_test,1062, isshuffle=False):\n",
    "    pred = get_prediction(classifier, sess, (x_batch, y_batch))\n",
    "    attn = get_attn_weight(classifier, sess, (x_batch, y_batch))\n",
    "    preds += pred.tolist()\n",
    "    attns += attn.tolist()\n",
    "    reals += y_batch.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({'sent':test_doc,'attn':attns, 'pred':preds, 'real':reals})\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df['real'] = test_df['real'].apply(lambda x: cols[x])\n",
    "test_df['pred'] = test_df['pred'].apply(lambda x: cols[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(test_df['real'],test_df['pred'],average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_df[test_df['real'] == test_df['pred']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
