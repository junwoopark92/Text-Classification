{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/1003874/anaconda/envs/lstm/lib/python3.6/site-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "from jamo import j2hcj, h2j\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from konlpy.tag import Twitter\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "twitter = Twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/1003874/anaconda/envs/lstm/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\n",
    "from tensorflow.contrib.rnn import BasicLSTMCell\n",
    "from utils.prepare_data import *\n",
    "import time\n",
    "from utils.model_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testcase_shuffle_data_load():\n",
    "    df = pd.read_csv('/Users/1003874/bsdev/Text-classification-with-CNN-RNN-with-Tensorflow/Ch01_Data_load/data/preprocessed_mart_digi_speech_act.csv')\n",
    "    df['clean_text'] = df['clean_text'].apply(lambda x:x.lower())\n",
    "    s = df['class'].value_counts()\n",
    "    cols = s[s>50].index.tolist()\n",
    "\n",
    "    sub_list = []\n",
    "    for col in cols:\n",
    "        sub_list.append(df[df['class'] == col])\n",
    "    df = pd.concat(sub_list, axis=0)\n",
    "    df = df.sample(len(df), random_state=10).reset_index(drop=True)\n",
    "\n",
    "    added_df = pd.read_csv('/Users/1003874/bsdev/Text-classification-with-CNN-RNN-with-Tensorflow/Ch01_Data_load/data/added_dataset.csv',sep='\\t')\n",
    "    added_df['clean_text'] = added_df['sent'].apply(lambda x:x.lower())\n",
    "\n",
    "    # class 제거\n",
    "    added_df = added_df[added_df['class'].apply(lambda x: True if x in cols else False)]\n",
    "    added_df.info()\n",
    "\n",
    "    # duplicated sent 제거\n",
    "    added_df = added_df[added_df['clean_text'].apply(lambda x: False if x in df['clean_text'].tolist() else True)]\n",
    "    added_df.info()\n",
    "\n",
    "    # train_df merge\n",
    "\n",
    "    size = len(df)\n",
    "    rate = 0.2\n",
    "    train_df = df.iloc[:int(size * (1 - rate))]\n",
    "    test_df = df.iloc[int(size * (1 - rate)):]\n",
    "\n",
    "    all_df = pd.concat([train_df[['clean_text', 'class']],\n",
    "                        test_df[['clean_text', 'class']],\n",
    "                        added_df[['clean_text', 'class']]])\n",
    "\n",
    "    all_df = all_df.sample(len(all_df), random_state=10)\n",
    "    all_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    label_df = pd.get_dummies(all_df['class'])\n",
    "\n",
    "    train_df = all_df.iloc[1062:]\n",
    "    test_df = all_df.iloc[:1062]\n",
    "\n",
    "    cols = label_df.columns\n",
    "\n",
    "    print(len(train_df), len(test_df))\n",
    "\n",
    "    TRAIN_DOC, TRAIN_LABEL, TEST_DOC, TEST_LABEL = \\\n",
    "        train_df['clean_text'].values, label_df.loc[train_df.index].values, test_df['clean_text'].values, label_df.loc[\n",
    "            test_df.index].values\n",
    "\n",
    "    return TRAIN_DOC, TRAIN_LABEL, TEST_DOC, TEST_LABEL, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testcase_add_data_load():\n",
    "    df = pd.read_csv('/Users/1003874/bsdev/Text-classification-with-CNN-RNN-with-Tensorflow/Ch01_Data_load/data/preprocessed_mart_digi_speech_act.csv')\n",
    "    df['clean_text'] = df['clean_text'].apply(lambda x:x.lower())\n",
    "    s = df['class'].value_counts()\n",
    "    cols = s[s>50].index.tolist()\n",
    "\n",
    "    sub_list = []\n",
    "    for col in cols:\n",
    "        sub_list.append(df[df['class'] == col])\n",
    "    df = pd.concat(sub_list, axis=0)\n",
    "    df = df.sample(len(df), random_state=10).reset_index(drop=True)\n",
    "\n",
    "    added_df = pd.read_csv(\n",
    "        '/Users/1003874/bsdev/Text-classification-with-CNN-RNN-with-Tensorflow/Ch01_Data_load/data/added_dataset.csv',\n",
    "        sep='\\t')\n",
    "    added_df['clean_text'] = added_df['sent'].apply(lambda x:x.lower())\n",
    "\n",
    "    # class 제거\n",
    "    added_df = added_df[added_df['class'].apply(lambda x: True if x in cols else False)]\n",
    "    added_df.info()\n",
    "\n",
    "    # duplicated sent 제거\n",
    "    added_df = added_df[added_df['clean_text'].apply(lambda x: False if x in df['clean_text'].tolist() else True)]\n",
    "    added_df.info()\n",
    "\n",
    "    # train_df merge\n",
    "\n",
    "    size = len(df)\n",
    "    rate = 0.2\n",
    "    train_df = df.iloc[:int(size * (1 - rate))]\n",
    "    test_df = df.iloc[int(size * (1 - rate)):]\n",
    "    train_df['setlabel'] = 1\n",
    "    test_df['setlabel'] = 0\n",
    "    added_df['setlabel'] = 2\n",
    "\n",
    "    all_df = pd.concat([train_df[['clean_text', 'class', 'setlabel']],\n",
    "                        test_df[['clean_text', 'class', 'setlabel']],\n",
    "                        added_df[['clean_text', 'class','setlabel']]])\n",
    "\n",
    "    all_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    label_df = pd.get_dummies(all_df['class'])\n",
    "\n",
    "    train_df = all_df[all_df['setlabel'] != 0]\n",
    "    test_df = all_df[all_df['setlabel'] == 0]\n",
    "\n",
    "    cols = label_df.columns\n",
    "\n",
    "    print(len(train_df), len(test_df))\n",
    "\n",
    "    TRAIN_DOC, TRAIN_LABEL, TEST_DOC, TEST_LABEL = \\\n",
    "        train_df['clean_text'].values, label_df.loc[train_df.index].values, test_df['clean_text'].values, label_df.loc[\n",
    "            test_df.index].values\n",
    "\n",
    "    return TRAIN_DOC, TRAIN_LABEL, TEST_DOC, TEST_LABEL, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def digi_data_load():\n",
    "    df = pd.read_csv('/Users/1003874/bsdev/Text-classification-with-CNN-RNN-with-Tensorflow/Ch01_Data_load/data/preprocessed_mart_digi_speech_act.csv')\n",
    "    df['clean_text'] = df['clean_text'].apply(lambda x:x.lower())\n",
    "    s = df['class'].value_counts()\n",
    "    cols = s[s>50].index.tolist()\n",
    "\n",
    "    sub_list = []\n",
    "    for col in cols:\n",
    "        sub_list.append(df[df['class'] == col])\n",
    "    df = pd.concat(sub_list, axis=0)\n",
    "    df = df.sample(len(df), random_state=10).reset_index(drop=True)\n",
    "\n",
    "    size = len(df)\n",
    "    rate = 0.2\n",
    "    train_df = df.iloc[:int(size*(1-rate))]\n",
    "    test_df = df.iloc[int(size*(1-rate)):]\n",
    "    label_df = pd.get_dummies(df['class'])\n",
    "    cols = label_df.columns\n",
    "    print(len(train_df), len(test_df))\n",
    "\n",
    "    TRAIN_DOC, TRAIN_LABEL, TEST_DOC, TEST_LABEL = \\\n",
    "        train_df['clean_text'].values, label_df.loc[train_df.index].values, test_df['clean_text'].values, label_df.loc[test_df.index].values\n",
    "\n",
    "    return TRAIN_DOC, TRAIN_LABEL, TEST_DOC, TEST_LABEL, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ABLSTM(object):\n",
    "    def __init__(self, config):\n",
    "        self.max_len = config[\"max_len\"]\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.vocab_size = config[\"vocab_size\"]\n",
    "        self.embedding_size = config[\"embedding_size\"]\n",
    "        self.n_class = config[\"n_class\"]\n",
    "        self.learning_rate = config[\"learning_rate\"]\n",
    "\n",
    "        # placeholder\n",
    "        self.x = tf.placeholder(tf.int32, [None, self.max_len])\n",
    "        self.label = tf.placeholder(tf.int32, [None])\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    def build_graph(self):\n",
    "        print(\"building graph\")\n",
    "        # Word embedding\n",
    "        embeddings_var = tf.Variable(tf.random_uniform([self.vocab_size, self.embedding_size], -1.0, 1.0),\n",
    "                                     trainable=True)\n",
    "        batch_embedded = tf.nn.embedding_lookup(embeddings_var, self.x)\n",
    "\n",
    "        rnn_outputs, _ = bi_rnn(BasicLSTMCell(self.hidden_size),\n",
    "                                BasicLSTMCell(self.hidden_size),\n",
    "                                inputs=batch_embedded, dtype=tf.float32)\n",
    "\n",
    "        fw_outputs, bw_outputs = rnn_outputs\n",
    "\n",
    "        W = tf.Variable(tf.random_normal([self.hidden_size], stddev=0.1))\n",
    "        H = fw_outputs + bw_outputs  # (batch_size, seq_len, HIDDEN_SIZE)\n",
    "        M = tf.tanh(H)  # M = tanh(H)  (batch_size, seq_len, HIDDEN_SIZE)\n",
    "\n",
    "        self.alpha = tf.nn.softmax(tf.reshape(tf.matmul(tf.reshape(M, [-1, self.hidden_size]),\n",
    "                                                        tf.reshape(W, [-1, 1])),\n",
    "                                              (-1, self.max_len)))  # batch_size x seq_len\n",
    "        r = tf.matmul(tf.transpose(H, [0, 2, 1]),\n",
    "                      tf.reshape(self.alpha, [-1, self.max_len, 1]))\n",
    "        r = tf.squeeze(r)\n",
    "        h_star = tf.tanh(r)  # (batch , HIDDEN_SIZE\n",
    "\n",
    "        h_drop = tf.nn.dropout(h_star, self.keep_prob)\n",
    "\n",
    "        # Fully connected layer（dense layer)\n",
    "        FC_W = tf.Variable(tf.truncated_normal([self.hidden_size, self.n_class], stddev=0.1))\n",
    "        FC_b = tf.Variable(tf.constant(0., shape=[self.n_class]))\n",
    "        y_hat = tf.nn.xw_plus_b(h_drop, FC_W, FC_b)\n",
    "\n",
    "        self.loss = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y_hat, labels=self.label))\n",
    "\n",
    "        # prediction\n",
    "        self.prediction = tf.argmax(tf.nn.softmax(y_hat), 1)\n",
    "\n",
    "        # optimization\n",
    "        loss_to_minimize = self.loss\n",
    "        tvars = tf.trainable_variables()\n",
    "        gradients = tf.gradients(loss_to_minimize, tvars, aggregation_method=tf.AggregationMethod.EXPERIMENTAL_TREE)\n",
    "        grads, global_norm = tf.clip_by_global_norm(gradients, 1.0)\n",
    "\n",
    "        self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        self.train_op = self.optimizer.apply_gradients(zip(grads, tvars), global_step=self.global_step,\n",
    "                                                       name='train_step')\n",
    "        print(\"graph built successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4244 1062\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DOC, TRAIN_LABEL, TEST_DOC, TEST_LABEL, cols = digi_data_load()\n",
    "#TRAIN_DOC, TRAIN_LABEL, TEST_DOC, TEST_LABEL, cols = testcase_add_data_load()\n",
    "#TRAIN_DOC, TRAIN_LABEL, TEST_DOC, TEST_LABEL, cols = testcase_shuffle_data_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_intent = np.apply_along_axis(np.argmax,1,TRAIN_LABEL)\n",
    "test_intent = np.apply_along_axis(np.argmax,1,TEST_LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.4 ms, sys: 1.65 ms, total: 18 ms\n",
      "Wall time: 21.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_doc = TRAIN_DOC.tolist()\n",
    "test_doc = TEST_DOC.tolist()\n",
    "\n",
    "#train_doc = pd.Series(list(map(lambda x: ' '.join(twitter.morphs(x)), train_doc)))\n",
    "#test_doc = pd.Series(list(map(lambda x: ' '.join(twitter.morphs(x)), test_doc)))\n",
    "train_doc = pd.Series(list(map(lambda x: ' '.join(list(x.replace(' ','ⓢ'))), train_doc)))\n",
    "test_doc = pd.Series(list(map(lambda x: ' '.join(list(x.replace(' ','ⓢ'))), test_doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = pd.Series(train_intent)\n",
    "y_test = pd.Series(test_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, tokenizer = \\\n",
    "        data_preprocessing_v2(train_doc, test_doc, max_len=128)\n",
    "vocab_size = tokenizer.num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 1,\n",
       " '1': 84,\n",
       " '2': 3,\n",
       " '4': 2,\n",
       " '5': 2,\n",
       " 'a': 82,\n",
       " 'b': 1,\n",
       " 'c': 6,\n",
       " 'd': 4,\n",
       " 'e': 6,\n",
       " 'f': 5,\n",
       " 'g': 1,\n",
       " 'h': 1,\n",
       " 'i': 38,\n",
       " 'k': 31,\n",
       " 'l': 4,\n",
       " 'm': 8,\n",
       " 'n': 4,\n",
       " 'o': 20,\n",
       " 'p': 57,\n",
       " 'q': 7,\n",
       " 'r': 4,\n",
       " 's': 64,\n",
       " 't': 25,\n",
       " 'u': 3,\n",
       " 'v': 27,\n",
       " 'w': 1,\n",
       " 'y': 25,\n",
       " 'ⓢ': 3546,\n",
       " '가': 732,\n",
       " '각': 6,\n",
       " '간': 43,\n",
       " '갈': 6,\n",
       " '감': 9,\n",
       " '갑': 5,\n",
       " '갔': 1,\n",
       " '강': 1,\n",
       " '갖': 2,\n",
       " '같': 30,\n",
       " '갚': 1,\n",
       " '개': 37,\n",
       " '객': 29,\n",
       " '갸': 1,\n",
       " '거': 170,\n",
       " '건': 172,\n",
       " '걸': 41,\n",
       " '검': 19,\n",
       " '겁': 5,\n",
       " '것': 42,\n",
       " '게': 322,\n",
       " '겟': 2,\n",
       " '겠': 38,\n",
       " '겨': 9,\n",
       " '격': 78,\n",
       " '결': 111,\n",
       " '경': 62,\n",
       " '계': 35,\n",
       " '고': 351,\n",
       " '곡': 1,\n",
       " '곤': 2,\n",
       " '골': 13,\n",
       " '곳': 8,\n",
       " '공': 25,\n",
       " '과': 10,\n",
       " '관': 51,\n",
       " '광': 6,\n",
       " '괜': 18,\n",
       " '교': 65,\n",
       " '구': 218,\n",
       " '국': 3,\n",
       " '군': 8,\n",
       " '굵': 1,\n",
       " '궁': 26,\n",
       " '권': 31,\n",
       " '귀': 4,\n",
       " '귄': 1,\n",
       " '규': 2,\n",
       " '균': 1,\n",
       " '그': 75,\n",
       " '근': 18,\n",
       " '글': 3,\n",
       " '금': 161,\n",
       " '급': 42,\n",
       " '긍': 1,\n",
       " '기': 172,\n",
       " '긴': 2,\n",
       " '길': 2,\n",
       " '까': 103,\n",
       " '깐': 2,\n",
       " '깡': 1,\n",
       " '깨': 1,\n",
       " '꺼': 6,\n",
       " '껀': 1,\n",
       " '께': 53,\n",
       " '꼈': 2,\n",
       " '꼬': 1,\n",
       " '꼭': 1,\n",
       " '꽤': 1,\n",
       " '꾸': 23,\n",
       " '꿀': 5,\n",
       " '꿈': 1,\n",
       " '꿔': 9,\n",
       " '꿨': 1,\n",
       " '뀌': 7,\n",
       " '끄': 2,\n",
       " '끔': 1,\n",
       " '끝': 4,\n",
       " '끼': 1,\n",
       " '나': 491,\n",
       " '난': 16,\n",
       " '날': 14,\n",
       " '남': 15,\n",
       " '납': 12,\n",
       " '났': 3,\n",
       " '낮': 2,\n",
       " '내': 143,\n",
       " '낸': 2,\n",
       " '낼': 3,\n",
       " '냈': 2,\n",
       " '냐': 33,\n",
       " '냥': 15,\n",
       " '너': 62,\n",
       " '넌': 16,\n",
       " '널': 4,\n",
       " '넘': 8,\n",
       " '넛': 1,\n",
       " '넣': 1,\n",
       " '네': 83,\n",
       " '넴': 1,\n",
       " '넹': 1,\n",
       " '녁': 2,\n",
       " '년': 4,\n",
       " '녕': 1,\n",
       " '노': 8,\n",
       " '놀': 1,\n",
       " '놉': 1,\n",
       " '농': 3,\n",
       " '높': 1,\n",
       " '놓': 4,\n",
       " '놔': 2,\n",
       " '놨': 1,\n",
       " '누': 20,\n",
       " '눈': 1,\n",
       " '눌': 1,\n",
       " '뉴': 9,\n",
       " '느': 19,\n",
       " '는': 526,\n",
       " '늘': 29,\n",
       " '늠': 1,\n",
       " '능': 160,\n",
       " '늦': 4,\n",
       " '늬': 1,\n",
       " '니': 277,\n",
       " '닉': 2,\n",
       " '닌': 13,\n",
       " '님': 8,\n",
       " '닙': 2,\n",
       " '다': 369,\n",
       " '단': 23,\n",
       " '달': 30,\n",
       " '담': 210,\n",
       " '답': 33,\n",
       " '당': 21,\n",
       " '대': 126,\n",
       " '더': 35,\n",
       " '덕': 1,\n",
       " '던': 37,\n",
       " '덜': 2,\n",
       " '데': 282,\n",
       " '델': 11,\n",
       " '뎃': 1,\n",
       " '도': 172,\n",
       " '돈': 18,\n",
       " '돌': 6,\n",
       " '동': 21,\n",
       " '돼': 42,\n",
       " '됀': 1,\n",
       " '됏': 1,\n",
       " '됐': 21,\n",
       " '되': 280,\n",
       " '된': 28,\n",
       " '될': 13,\n",
       " '됨': 13,\n",
       " '됩': 4,\n",
       " '됬': 1,\n",
       " '두': 15,\n",
       " '둘': 3,\n",
       " '뒤': 1,\n",
       " '드': 189,\n",
       " '득': 1,\n",
       " '든': 10,\n",
       " '들': 30,\n",
       " '듯': 2,\n",
       " '등': 82,\n",
       " '디': 172,\n",
       " '딜': 10,\n",
       " '딤': 1,\n",
       " '딧': 3,\n",
       " '딨': 3,\n",
       " '딩': 3,\n",
       " '따': 4,\n",
       " '딱': 1,\n",
       " '딴': 6,\n",
       " '때': 27,\n",
       " '땜': 1,\n",
       " '떠': 9,\n",
       " '떡': 5,\n",
       " '떤': 33,\n",
       " '떨': 3,\n",
       " '떻': 188,\n",
       " '또': 25,\n",
       " '똑': 5,\n",
       " '똥': 1,\n",
       " '뚝': 1,\n",
       " '뚱': 2,\n",
       " '뜨': 13,\n",
       " '뜰': 1,\n",
       " '띄': 1,\n",
       " '라': 113,\n",
       " '락': 28,\n",
       " '란': 6,\n",
       " '람': 55,\n",
       " '랍': 2,\n",
       " '랑': 16,\n",
       " '래': 42,\n",
       " '랜': 9,\n",
       " '램': 4,\n",
       " '랩': 1,\n",
       " '량': 9,\n",
       " '러': 28,\n",
       " '럭': 1,\n",
       " '런': 4,\n",
       " '럼': 18,\n",
       " '럽': 16,\n",
       " '렀': 1,\n",
       " '렇': 1,\n",
       " '레': 4,\n",
       " '려': 174,\n",
       " '력': 20,\n",
       " '련': 30,\n",
       " '렴': 25,\n",
       " '렵': 2,\n",
       " '렸': 4,\n",
       " '령': 8,\n",
       " '례': 1,\n",
       " '로': 208,\n",
       " '록': 84,\n",
       " '롬': 1,\n",
       " '롯': 4,\n",
       " '롱': 1,\n",
       " '료': 40,\n",
       " '루': 6,\n",
       " '룬': 1,\n",
       " '류': 32,\n",
       " '르': 15,\n",
       " '른': 91,\n",
       " '를': 72,\n",
       " '름': 17,\n",
       " '릅': 1,\n",
       " '리': 116,\n",
       " '릭': 2,\n",
       " '린': 7,\n",
       " '릴': 19,\n",
       " '림': 3,\n",
       " '립': 20,\n",
       " '마': 84,\n",
       " '막': 1,\n",
       " '만': 95,\n",
       " '많': 26,\n",
       " '말': 46,\n",
       " '맘': 7,\n",
       " '맛': 1,\n",
       " '망': 3,\n",
       " '맞': 16,\n",
       " '맡': 3,\n",
       " '매': 179,\n",
       " '맴': 4,\n",
       " '머': 13,\n",
       " '먹': 11,\n",
       " '먼': 4,\n",
       " '멀': 1,\n",
       " '멈': 2,\n",
       " '메': 35,\n",
       " '멤': 26,\n",
       " '멥': 3,\n",
       " '며': 4,\n",
       " '면': 98,\n",
       " '멸': 1,\n",
       " '명': 20,\n",
       " '몇': 18,\n",
       " '모': 45,\n",
       " '목': 23,\n",
       " '몬': 2,\n",
       " '몰': 8,\n",
       " '못': 52,\n",
       " '무': 92,\n",
       " '묶': 5,\n",
       " '문': 362,\n",
       " '묻': 2,\n",
       " '물': 115,\n",
       " '뭐': 113,\n",
       " '뭔': 25,\n",
       " '뭘': 19,\n",
       " '뮈': 1,\n",
       " '뮌': 1,\n",
       " '미': 28,\n",
       " '민': 9,\n",
       " '믿': 1,\n",
       " '밀': 9,\n",
       " '밌': 4,\n",
       " '및': 1,\n",
       " '바': 83,\n",
       " '박': 1,\n",
       " '밖': 4,\n",
       " '반': 69,\n",
       " '받': 134,\n",
       " '발': 53,\n",
       " '밤': 3,\n",
       " '밥': 4,\n",
       " '방': 75,\n",
       " '배': 193,\n",
       " '백': 22,\n",
       " '뱅': 1,\n",
       " '버': 37,\n",
       " '번': 133,\n",
       " '벌': 1,\n",
       " '법': 93,\n",
       " '베': 2,\n",
       " '벤': 14,\n",
       " '벨': 1,\n",
       " '벽': 2,\n",
       " '변': 76,\n",
       " '별': 13,\n",
       " '보': 186,\n",
       " '복': 10,\n",
       " '본': 32,\n",
       " '볼': 46,\n",
       " '봄': 1,\n",
       " '봅': 2,\n",
       " '봇': 42,\n",
       " '봉': 1,\n",
       " '봐': 30,\n",
       " '봤': 6,\n",
       " '뵐': 1,\n",
       " '부': 105,\n",
       " '북': 2,\n",
       " '분': 13,\n",
       " '불': 96,\n",
       " '붕': 1,\n",
       " '붙': 1,\n",
       " '뷰': 8,\n",
       " '븀': 1,\n",
       " '브': 12,\n",
       " '블': 1,\n",
       " '비': 99,\n",
       " '빌': 1,\n",
       " '빙': 6,\n",
       " '빠': 12,\n",
       " '빨': 11,\n",
       " '빼': 2,\n",
       " '뽑': 1,\n",
       " '뿌': 1,\n",
       " '뿐': 3,\n",
       " '뿡': 1,\n",
       " '쁘': 2,\n",
       " '삐': 1,\n",
       " '사': 288,\n",
       " '삭': 30,\n",
       " '산': 20,\n",
       " '살': 32,\n",
       " '삼': 5,\n",
       " '삿': 2,\n",
       " '샀': 18,\n",
       " '상': 458,\n",
       " '새': 7,\n",
       " '색': 14,\n",
       " '생': 23,\n",
       " '샵': 1,\n",
       " '서': 210,\n",
       " '석': 3,\n",
       " '선': 37,\n",
       " '설': 20,\n",
       " '성': 28,\n",
       " '세': 127,\n",
       " '센': 18,\n",
       " '셀': 3,\n",
       " '셔': 3,\n",
       " '션': 6,\n",
       " '셜': 1,\n",
       " '셨': 7,\n",
       " '소': 196,\n",
       " '속': 14,\n",
       " '손': 4,\n",
       " '송': 195,\n",
       " '쇼': 12,\n",
       " '수': 185,\n",
       " '순': 3,\n",
       " '술': 1,\n",
       " '쉬': 25,\n",
       " '쉽': 24,\n",
       " '슈': 1,\n",
       " '스': 54,\n",
       " '슨': 12,\n",
       " '습': 52,\n",
       " '슷': 2,\n",
       " '승': 9,\n",
       " '시': 218,\n",
       " '식': 10,\n",
       " '신': 78,\n",
       " '실': 15,\n",
       " '싫': 6,\n",
       " '심': 14,\n",
       " '십': 11,\n",
       " '싱': 1,\n",
       " '싶': 82,\n",
       " '싸': 39,\n",
       " '싼': 25,\n",
       " '써': 11,\n",
       " '썼': 1,\n",
       " '쏴': 1,\n",
       " '쓰': 23,\n",
       " '쓸': 10,\n",
       " '씀': 1,\n",
       " '씁': 1,\n",
       " '씨': 4,\n",
       " '아': 247,\n",
       " '안': 346,\n",
       " '않': 37,\n",
       " '알': 104,\n",
       " '암': 1,\n",
       " '앗': 1,\n",
       " '았': 24,\n",
       " '앞': 1,\n",
       " '애': 3,\n",
       " '액': 23,\n",
       " '앱': 1,\n",
       " '야': 82,\n",
       " '약': 6,\n",
       " '얌': 1,\n",
       " '양': 1,\n",
       " '얘': 5,\n",
       " '어': 658,\n",
       " '억': 2,\n",
       " '언': 129,\n",
       " '얼': 48,\n",
       " '엄': 2,\n",
       " '업': 32,\n",
       " '없': 113,\n",
       " '엇': 10,\n",
       " '었': 20,\n",
       " '엉': 3,\n",
       " '에': 259,\n",
       " '엔': 3,\n",
       " '엠': 2,\n",
       " '여': 103,\n",
       " '역': 32,\n",
       " '연': 72,\n",
       " '열': 5,\n",
       " '염': 2,\n",
       " '였': 3,\n",
       " '영': 16,\n",
       " '예': 19,\n",
       " '옛': 1,\n",
       " '오': 129,\n",
       " '온': 3,\n",
       " '올': 30,\n",
       " '옵': 6,\n",
       " '와': 50,\n",
       " '완': 11,\n",
       " '왓': 3,\n",
       " '왔': 26,\n",
       " '왜': 52,\n",
       " '외': 10,\n",
       " '요': 1263,\n",
       " '욕': 1,\n",
       " '욜': 10,\n",
       " '욤': 2,\n",
       " '용': 241,\n",
       " '우': 16,\n",
       " '운': 20,\n",
       " '울': 1,\n",
       " '움': 7,\n",
       " '웃': 6,\n",
       " '워': 10,\n",
       " '원': 68,\n",
       " '월': 11,\n",
       " '웠': 1,\n",
       " '웰': 9,\n",
       " '위': 14,\n",
       " '유': 18,\n",
       " '율': 1,\n",
       " '으': 89,\n",
       " '은': 278,\n",
       " '을': 190,\n",
       " '음': 41,\n",
       " '응': 10,\n",
       " '읗': 1,\n",
       " '의': 188,\n",
       " '이': 822,\n",
       " '인': 398,\n",
       " '일': 141,\n",
       " '임': 7,\n",
       " '입': 130,\n",
       " '잇': 12,\n",
       " '있': 227,\n",
       " '잊': 3,\n",
       " '자': 139,\n",
       " '작': 12,\n",
       " '잖': 2,\n",
       " '잘': 52,\n",
       " '잠': 4,\n",
       " '잡': 1,\n",
       " '장': 64,\n",
       " '재': 59,\n",
       " '잼': 1,\n",
       " '저': 85,\n",
       " '적': 70,\n",
       " '전': 111,\n",
       " '절': 10,\n",
       " '점': 35,\n",
       " '접': 17,\n",
       " '정': 133,\n",
       " '제': 381,\n",
       " '젠': 2,\n",
       " '젤': 4,\n",
       " '져': 15,\n",
       " '졌': 11,\n",
       " '조': 32,\n",
       " '족': 1,\n",
       " '존': 8,\n",
       " '졸': 1,\n",
       " '좀': 78,\n",
       " '종': 17,\n",
       " '좋': 54,\n",
       " '좌': 8,\n",
       " '죄': 1,\n",
       " '죠': 45,\n",
       " '주': 352,\n",
       " '준': 14,\n",
       " '줄': 17,\n",
       " '줌': 1,\n",
       " '중': 54,\n",
       " '줘': 62,\n",
       " '줭': 1,\n",
       " '쥬': 1,\n",
       " '즈': 7,\n",
       " '즉': 1,\n",
       " '즐': 1,\n",
       " '즘': 2,\n",
       " '증': 99,\n",
       " '지': 369,\n",
       " '직': 31,\n",
       " '진': 13,\n",
       " '질': 17,\n",
       " '짐': 1,\n",
       " '집': 5,\n",
       " '짓': 1,\n",
       " '징': 1,\n",
       " '짜': 11,\n",
       " '짝': 2,\n",
       " '째': 3,\n",
       " '쩌': 2,\n",
       " '쩝': 1,\n",
       " '쪽': 2,\n",
       " '쭈': 1,\n",
       " '쭉': 1,\n",
       " '쭙': 1,\n",
       " '쭤': 8,\n",
       " '쯤': 15,\n",
       " '찌': 4,\n",
       " '차': 86,\n",
       " '착': 34,\n",
       " '찬': 5,\n",
       " '찮': 15,\n",
       " '참': 3,\n",
       " '찻': 1,\n",
       " '창': 13,\n",
       " '찾': 66,\n",
       " '찿': 1,\n",
       " '채': 6,\n",
       " '책': 2,\n",
       " '챗': 28,\n",
       " '챙': 4,\n",
       " '처': 26,\n",
       " '척': 2,\n",
       " '천': 31,\n",
       " '첨': 7,\n",
       " '첫': 5,\n",
       " '청': 58,\n",
       " '체': 19,\n",
       " '쳇': 1,\n",
       " '쳐': 4,\n",
       " '초': 8,\n",
       " '최': 30,\n",
       " '추': 53,\n",
       " '출': 27,\n",
       " '춤': 1,\n",
       " '충': 5,\n",
       " '춰': 2,\n",
       " '취': 150,\n",
       " '치': 14,\n",
       " '친': 12,\n",
       " '칠': 2,\n",
       " '침': 1,\n",
       " '카': 109,\n",
       " '칼': 1,\n",
       " '캐': 48,\n",
       " '커': 1,\n",
       " '컨': 4,\n",
       " '컬': 3,\n",
       " '컴': 13,\n",
       " '케': 26,\n",
       " '켐': 1,\n",
       " '켓': 1,\n",
       " '켜': 5,\n",
       " '켰': 6,\n",
       " '코': 10,\n",
       " '콕': 1,\n",
       " '콘': 4,\n",
       " '콜': 1,\n",
       " '콤': 3,\n",
       " '쿠': 362,\n",
       " '큐': 4,\n",
       " '크': 8,\n",
       " '큰': 3,\n",
       " '클': 6,\n",
       " '큼': 2,\n",
       " '키': 6,\n",
       " '킨': 1,\n",
       " '킬': 1,\n",
       " '킹': 1,\n",
       " '타': 4,\n",
       " '탁': 64,\n",
       " '탈': 5,\n",
       " '탑': 2,\n",
       " '태': 7,\n",
       " '택': 83,\n",
       " '터': 53,\n",
       " '털': 21,\n",
       " '테': 9,\n",
       " '텔': 2,\n",
       " '템': 6,\n",
       " '텡': 1,\n",
       " '토': 13,\n",
       " '톡': 37,\n",
       " '톱': 1,\n",
       " '통': 47,\n",
       " '퇴': 9,\n",
       " '툼': 4,\n",
       " '퉁': 2,\n",
       " '튜': 1,\n",
       " '트': 82,\n",
       " '특': 7,\n",
       " '튼': 4,\n",
       " '틀': 6,\n",
       " '티': 19,\n",
       " '팀': 1,\n",
       " '팅': 6,\n",
       " '파': 12,\n",
       " '판': 24,\n",
       " '팔': 11,\n",
       " '팝': 4,\n",
       " '팡': 1,\n",
       " '패': 5,\n",
       " '팩': 6,\n",
       " '팸': 1,\n",
       " '퍄': 1,\n",
       " '퍼': 1,\n",
       " '페': 40,\n",
       " '펫': 1,\n",
       " '펴': 4,\n",
       " '편': 7,\n",
       " '평': 4,\n",
       " '포': 48,\n",
       " '폰': 378,\n",
       " '표': 4,\n",
       " '푸': 3,\n",
       " '풀': 2,\n",
       " '품': 428,\n",
       " '퓨': 6,\n",
       " '프': 17,\n",
       " '픈': 2,\n",
       " '플': 5,\n",
       " '픔': 1,\n",
       " '피': 10,\n",
       " '픽': 12,\n",
       " '핀': 2,\n",
       " '필': 17,\n",
       " '핑': 12,\n",
       " '하': 349,\n",
       " '학': 2,\n",
       " '한': 225,\n",
       " '할': 211,\n",
       " '함': 22,\n",
       " '합': 32,\n",
       " '핫': 4,\n",
       " '항': 10,\n",
       " '해': 280,\n",
       " '핸': 6,\n",
       " '햇': 5,\n",
       " '했': 100,\n",
       " '행': 13,\n",
       " '햐': 1,\n",
       " '향': 2,\n",
       " '햬': 1,\n",
       " '허': 3,\n",
       " '헌': 1,\n",
       " '헐': 1,\n",
       " '험': 1,\n",
       " '헤': 1,\n",
       " '현': 13,\n",
       " '협': 2,\n",
       " '형': 2,\n",
       " '혜': 38,\n",
       " '호': 73,\n",
       " '혹': 8,\n",
       " '혼': 2,\n",
       " '홈': 4,\n",
       " '화': 107,\n",
       " '확': 87,\n",
       " '환': 156,\n",
       " '활': 1,\n",
       " '황': 2,\n",
       " '회': 48,\n",
       " '획': 1,\n",
       " '횟': 1,\n",
       " '효': 4,\n",
       " '후': 19,\n",
       " '훼': 1,\n",
       " '휴': 9,\n",
       " '히': 14,\n",
       " '힌': 1,\n",
       " '힘': 9}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size:  1062\n",
      "building graph\n",
      "graph built successfully!\n"
     ]
    }
   ],
   "source": [
    "x_dev = x_test\n",
    "y_dev = y_test\n",
    "dev_size = len(x_dev)\n",
    "test_size = len(x_test)\n",
    "print(\"Validation Size: \", dev_size)\n",
    "\n",
    "config = {\n",
    "    \"max_len\": 64,\n",
    "    \"hidden_size\": 64,\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"embedding_size\": 128,\n",
    "    \"n_class\": 33,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"batch_size\": 4,\n",
    "    \"train_epoch\": 20\n",
    "}\n",
    "\n",
    "classifier = ABLSTM(config)\n",
    "classifier.build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 start !\n",
      "Train Epoch time:  625.326 s\n",
      "validation accuracy: 0.333 \n",
      "Epoch 2 start !\n",
      "Train Epoch time:  621.160 s\n",
      "validation accuracy: 0.442 \n",
      "Epoch 3 start !\n",
      "Train Epoch time:  624.311 s\n",
      "validation accuracy: 0.492 \n",
      "Epoch 4 start !\n",
      "Train Epoch time:  642.462 s\n",
      "validation accuracy: 0.512 \n",
      "Epoch 5 start !\n",
      "Train Epoch time:  632.748 s\n",
      "validation accuracy: 0.533 \n",
      "Epoch 6 start !\n",
      "Train Epoch time:  642.512 s\n",
      "validation accuracy: 0.558 \n",
      "Epoch 7 start !\n",
      "Train Epoch time:  645.349 s\n",
      "validation accuracy: 0.582 \n",
      "Epoch 8 start !\n",
      "Train Epoch time:  601.014 s\n",
      "validation accuracy: 0.593 \n",
      "Epoch 9 start !\n",
      "Train Epoch time:  596.741 s\n",
      "validation accuracy: 0.597 \n",
      "Epoch 10 start !\n",
      "Train Epoch time:  562.877 s\n",
      "validation accuracy: 0.605 \n",
      "Epoch 11 start !\n",
      "Train Epoch time:  584.656 s\n",
      "validation accuracy: 0.607 \n",
      "Epoch 12 start !\n",
      "Train Epoch time:  606.369 s\n",
      "validation accuracy: 0.612 \n",
      "Epoch 13 start !\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "dev_batch = (x_dev, y_dev)\n",
    "start = time.time()\n",
    "for e in range(config[\"train_epoch\"]):\n",
    "\n",
    "    t0 = time.time()\n",
    "    print(\"Epoch %d start !\" % (e + 1))\n",
    "    for x_batch, y_batch in fill_feed_dict(x_train, y_train, config[\"batch_size\"]):\n",
    "        return_dict = run_train_step(classifier, sess, (x_batch, y_batch))\n",
    "        attn = get_attn_weight(classifier, sess, (x_batch, y_batch))\n",
    "        # plot the attention weight\n",
    "        # print(np.reshape(attn, (config[\"batch_size\"], config[\"max_len\"])))\n",
    "    t1 = time.time()\n",
    "\n",
    "    print(\"Train Epoch time:  %.3f s\" % (t1 - t0))\n",
    "    dev_acc = run_eval_step(classifier, sess, dev_batch)\n",
    "    print(\"validation accuracy: %.3f \" % dev_acc)\n",
    "\n",
    "print(\"Training finished, time consumed : \", time.time() - start, \" s\")\n",
    "print(\"Start evaluating:  \\n\")\n",
    "cnt = 0\n",
    "test_acc = 0\n",
    "for x_batch, y_batch in fill_feed_dict(x_test, y_test, config[\"batch_size\"]):\n",
    "    acc = run_eval_step(classifier, sess, (x_batch, y_batch))\n",
    "    test_acc += acc\n",
    "    cnt += 1\n",
    "\n",
    "print(\"Test accuracy : %f %%\" % (test_acc / cnt * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "reals = []\n",
    "attns = []\n",
    "for x_batch, y_batch in fill_feed_dict(x_test, y_test,1062, isshuffle=False):\n",
    "    pred = get_prediction(classifier, sess, (x_batch, y_batch))\n",
    "    attn = get_attn_weight(classifier, sess, (x_batch, y_batch))\n",
    "    preds += pred.tolist()\n",
    "    attns += attn.tolist()\n",
    "    reals += y_batch.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({'sent':test_doc,'attn':attns, 'pred':preds, 'real':reals})\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df['real'] = test_df['real'].apply(lambda x: cols[x])\n",
    "test_df['pred'] = test_df['pred'].apply(lambda x: cols[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1_score(test_df['real'],test_df['pred'],average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_df[test_df['real'] == test_df['pred']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ㄱ ㅏ ㄴ ㅡ ⓢ ㄷ ㅏ ㄹ ㅏ ㅇ'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(list('ㄱㅏㄴㅡ ㄷㅏㄹㅏㅇ'.replace(' ', 'ⓢ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_preprocessing_v2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
